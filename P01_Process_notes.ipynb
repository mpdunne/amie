{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P01: Process notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 . Basic setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "##########\n",
    "# Config #\n",
    "##########\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "DATA_FOLDER = \"data\"\n",
    "VOCAB_OUTPUT = \"vocab.csv\"\n",
    "\n",
    "###################\n",
    "# Vocab extractor #\n",
    "###################\n",
    "\n",
    "def extract_vocab_from_notes(notes_text):\n",
    "    prompt = f\"\"\"\n",
    "Here are my messy French notes from lessons. Note that there might not actually be any French in here! If it looks like there's no vocab, just return an empty string. Sometimes there isn't—some files aren't relevant.\n",
    "\n",
    "But if there is, please extract each French word/concept/phrase.  \n",
    "For each one, return JUST the word/concept/phrase—but if there's a specific or unusual English usage (for example an ambiguity), add that in brackets after the word/phrase.\n",
    "\n",
    "Return a \\\\n-separated string. Each word/phrase on its own line. NO EXTRA TEXT. I'll process it later.\n",
    "\n",
    "Here are my notes:\n",
    "{notes_text}\n",
    "\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful French language assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"souviens  \\nproche de chez moi  \\nC'était une vraie bénédiction  \\nautrice  \\nbonne note  \\nconfus (confusing)  \\nl'info de trop  \\nc'est tres lent  \\nça va lentement  \\nelle m'a demandé  \\non a eu  \\nje ne la laisserai gagner  \\nmechant  \\nterminé  \\nj'ai rien de prevu  \\nruisseau  \\nabri (habitat)  \\nparmi (entre)  \\nrapide (fast)  \\nvite (quickly)  \\nbrûlant (boiling)  \\nfragile  \\nsolide  \\nsombre  \\nilluminé  \\nclair  \\npauvre  \\nriche  \\naisé  \\ncruel  \\ncharitable  \\nempathique  \\nbienveillant  \\nrugueux  \\ndoux  \\ndouce  \\nagité  \\ncalme  \\nsale  \\npropre  \\nfermé  \\nouvert  \\nmetier (trades)  \\narchitecte  \\ninstitutrice  \\ninst  \\nmason (builder)  \\ncomptable\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '''souviens\n",
    "\n",
    "proche de chez moi\n",
    "\n",
    "C'était une vraie bénédiction\n",
    "autrice / autrice\n",
    "\n",
    "bonne note\n",
    "\n",
    "confus - confusing\n",
    "\n",
    "l'info de trop\n",
    "\n",
    "c'est tres lent\n",
    "ça va lentement\n",
    "\n",
    "elle m'a demandé\n",
    "\n",
    "on a eu\n",
    "\n",
    "je ne la laisserai gagner\n",
    "mechant\n",
    "\n",
    "terminé\n",
    "\n",
    "Don Juan\n",
    "\n",
    "j'ai rien de prevu\n",
    "\n",
    "ruisseau\n",
    "\n",
    "abri - habitat\n",
    "\n",
    "parmi - entre\n",
    "\n",
    "rapide - fast\n",
    "vite - quickly\n",
    "\n",
    "brûlant - boiling\n",
    "\n",
    "fragile / solide\n",
    "sombre / illuminé / clair\n",
    "pauvre / riche / aisé\n",
    "\n",
    "cruel / charitable / empathique / bienveillant\n",
    "\n",
    "rugueux / doux / douce\n",
    "\n",
    "agité / calme\n",
    "\n",
    "sale / propre\n",
    "fermé / ouvert\n",
    "\n",
    "metier - trades\n",
    "\n",
    "architecte\n",
    "\n",
    "institutrice / inst\n",
    "\n",
    "mason - builder\n",
    "\n",
    "- -> comptable'''\n",
    "\n",
    "content = extract_vocab_from_notes(text)\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['souviens',\n",
       " 'proche de chez moi',\n",
       " \"C'était une vraie bénédiction\",\n",
       " 'autrice',\n",
       " 'bonne note',\n",
       " 'confus (confusing)',\n",
       " \"l'info de trop\",\n",
       " \"c'est tres lent\",\n",
       " 'ça va lentement',\n",
       " \"elle m'a demandé\",\n",
       " 'on a eu',\n",
       " 'je ne la laisserai gagner',\n",
       " 'mechant',\n",
       " 'terminé',\n",
       " \"j'ai rien de prevu\",\n",
       " 'ruisseau',\n",
       " 'abri (habitat)',\n",
       " 'parmi (entre)',\n",
       " 'rapide (fast)',\n",
       " 'vite (quickly)',\n",
       " 'brûlant (boiling)',\n",
       " 'fragile',\n",
       " 'solide',\n",
       " 'sombre',\n",
       " 'illuminé',\n",
       " 'clair',\n",
       " 'pauvre',\n",
       " 'riche',\n",
       " 'aisé',\n",
       " 'cruel',\n",
       " 'charitable',\n",
       " 'empathique',\n",
       " 'bienveillant',\n",
       " 'rugueux',\n",
       " 'doux',\n",
       " 'douce',\n",
       " 'agité',\n",
       " 'calme',\n",
       " 'sale',\n",
       " 'propre',\n",
       " 'fermé',\n",
       " 'ouvert',\n",
       " 'metier (trades)',\n",
       " 'architecte',\n",
       " 'institutrice',\n",
       " 'inst',\n",
       " 'mason (builder)',\n",
       " 'comptable']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = [x.strip() for x in content.split('\\n')]\n",
    "items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read them all and save the results..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep a track of stuff that we've seen..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data/caroline-2025-07-08/Week 25 9ea18f08c5214151a7ec55b20f4f1102.md...\n",
      "Loading data/caroline-2025-07-08/Week 27 21c33c653adb80ec9390fe7bda0a492a.md...\n",
      "Loading data/caroline-2025-07-08/Week 44 13033c653adb80c8bbc3ed592a702628.md...\n",
      "Loading data/caroline-2025-07-08/Week 32 cabcd9762cbe40f1be8b8c6a3ada84b1.md...\n",
      "Loading data/caroline-2025-07-08/Week 45 13733c653adb80f589fcd78f4e998407.md...\n",
      "Loading data/caroline-2025-07-08/Week 10 1b333c653adb8040be5de722982e9201.md...\n",
      "Loading data/caroline-2025-07-08/Week 48 14933c653adb80de9783eeaf64a419e1.md...\n",
      "Loading data/caroline-2025-07-08/Week 28 2adc4fb095ef44f6826005e294c427d4.md...\n",
      "Loading data/caroline-2025-07-08/Week 3 17b33c653adb8061b488fc2196c77287.md...\n",
      "Loading data/caroline-2025-07-08/Week 46 13c33c653adb80e69df6d47730617b37.md...\n",
      "Loading data/caroline-2025-07-08/Week 10 d8f31d450a7e4c4f824aa1416b584375.md...\n",
      "Loading data/caroline-2025-07-08/Week 17 1e433c653adb80d5b39ccc771ae89f7c.md...\n",
      "Loading data/caroline-2025-07-08/Week 41 11633c653adb80f9bf8ad1da47addef6.md...\n",
      "Loading data/caroline-2025-07-08/Week 39 60b904c9656a4b2b81eef51455e1fb70.md...\n",
      "Loading data/caroline-2025-07-08/Week 7 19133c653adb80ce9d2ccbfcbfda3feb.md...\n",
      "Loading data/caroline-2025-07-08/Week 4 18233c653adb807e88b4ca72f5dc4d03.md...\n",
      "Loading data/caroline-2025-07-08/Week 24 20733c653adb80c9a1cbce984cba791c.md...\n",
      "Loading data/caroline-2025-07-08/Week 21 f8737cb2f10a4dec888606d6249c7eb0.md...\n",
      "Loading data/caroline-2025-07-08/Week 6 19133c653adb80f4a6c7c290b6e33599.md...\n",
      "Loading data/caroline-2025-07-08/Week 18 1e433c653adb8045adbdedd71f0ce3ab.md...\n",
      "Loading data/caroline-2025-07-08/Week 26 21633c653adb80238064cc9dc71f33e3.md...\n",
      "Loading data/caroline-2025-07-08/Week 12 1bc33c653adb80798d9ef5c414739f46.md...\n",
      "Loading data/caroline-2025-07-08/Week 19 8be727bfffe24e2abb43291f52cc16c0.md...\n",
      "Loading data/caroline-2025-07-08/Week 11 1b333c653adb80c1a212c216fa929ddb.md...\n",
      "Loading data/caroline-2025-07-08/Week 16 1d633c653adb805bb7b0cb9379a301dc.md...\n",
      "Loading data/caroline-2025-07-08/Week 14 67eb4ab40ea5411b8fe6fd187f9d5e7f.md...\n",
      "Loading data/caroline-2025-07-08/Week 23 d6943e384dea457da699525ea4080345.md...\n",
      "Loading data/caroline-2025-07-08/Week 16 db0f5cb383e74ce5bea7b037b5816efd.md...\n",
      "Loading data/caroline-2025-07-08/Week 42 12033c653adb8053b67ae5647f9f9045.md...\n",
      "Loading data/caroline-2025-07-08/Week 11 1c567282d1674644aa596b8d6eae016c.md...\n",
      "Loading data/caroline-2025-07-08/Week 17 cb7af1fa3a7f40bc8ec80353e74593f3.md...\n",
      "Loading data/caroline-2025-07-08/Week 15 1d033c653adb80e5a35fcbc0976e8941.md...\n",
      "Loading data/caroline-2025-07-08/Week 5 18933c653adb80b78731fd370af20837.md...\n",
      "Loading data/caroline-2025-07-08/Week 37 f42417c2b5f3462382afef0d2006845a.md...\n",
      "Loading data/caroline-2025-07-08/Week 24 f64f12cf2f0f46a7913b8d4bf959abb3.md...\n",
      "Loading data/caroline-2025-07-08/Week 13 8564264985e94cd9b4ecb1eb14365ebd.md...\n",
      "Loading data/caroline-2025-07-08/Week 23 20733c653adb803b9731c1ac03e08527.md...\n",
      "Loading data/caroline-2025-07-08/Week 27 1469ba0c3f0e484e8eff6f030d19505b.md...\n",
      "Loading data/caroline-2025-07-08/French with Caroline d4f6e7c5849d4a38a0da21f064b8bd6e.csv...\n",
      "Loading data/caroline-2025-07-08/Week 13 1c133c653adb803192fafc79a06906ca.md...\n",
      "Loading data/caroline-2025-07-08/Week 20 3f0e93741d6541098916bc56160d37c9.md...\n",
      "Loading data/caroline-2025-07-08/Week 19 1e433c653adb80ca8e21d6de6f8b61a2.md...\n",
      "Loading data/caroline-2025-07-08/Week 36 76b53cf56d0b45f38a89b66416e7be3a.md...\n",
      "Loading data/caroline-2025-07-08/Week 35 0276954682594e96995a57136343f1e0.md...\n",
      "Loading data/caroline-2025-07-08/French with Caroline d4f6e7c5849d4a38a0da21f064b8bd6e_all.csv...\n",
      "Loading data/caroline-2025-07-08/Week 49 14933c653adb800298c6dfa333b0d4cf.md...\n",
      "Loading data/caroline-2025-07-08/Week 14 1c833c653adb802b8c4adc2a7db24382.md...\n",
      "Loading data/caroline-2025-07-08/Week 40 f9c1730a338241a1ba4e2dcfeb501efd.md...\n",
      "Loading data/caroline-2025-07-08/Week 47 14933c653adb8097a06ccd8dc27d644f.md...\n",
      "Loading data/caroline-2025-07-08/Week 15 d73dd43844a148df822e208543a30466.md...\n",
      "Loading data/caroline-2025-07-08/Week 34 0553eff543e0453493d6b97975828b8d.md...\n",
      "Loading data/caroline-2025-07-08/Week 31 786db394c7474c59873d7e7cb261fe74.md...\n",
      "Loading data/caroline-2025-07-08/Week 26 f48284ef81b74a9cb13a18e452ebffc3.md...\n",
      "Loading data/caroline-2025-07-08/Week 18 1053e63d3cc14c068408fdd45ab4be8a.md...\n",
      "Loading data/caroline-2025-07-08/Week 29 22a33c653adb8060a1faf0858ef6324f.md...\n",
      "Loading data/caroline-2025-07-08/Week 33 2a74614203e04531a2abe74eb08bd218.md...\n",
      "Loading data/caroline-2025-07-08/Week 43 12233c653adb809abff5c5b0a5f71d0b.md...\n",
      "Loading data/caroline-2025-07-08/Week 12 a4d5472084ff4290899944964968a489.md...\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "processed_suffix = '.processed.pkl'\n",
    "data_dir = 'data'\n",
    "\n",
    "all_vocab = []\n",
    "\n",
    "for file in glob.glob(os.path.join(data_dir, '**/*'), recursive=True):\n",
    "    \n",
    "    if file.endswith(('.txt', '.md', '.csv')):\n",
    "        processed_file = f'{file}{processed_suffix}'\n",
    "        \n",
    "        if not os.path.exists(processed_file):\n",
    "            print(f'Reading {file}')\n",
    "            content = open(file, 'r').read()\n",
    "            content_processed = extract_vocab_from_notes(content)\n",
    "            print(content_processed)\n",
    "            pickle.dump(content_processed, open(processed_file, 'wb'))\n",
    "\n",
    "        else:\n",
    "            print(f'Loading {file}...')\n",
    "            content_processed = pickle.load(open(processed_file, 'rb'))\n",
    "\n",
    "        vocab = [x.strip() for x in content_processed.split('\\n')]\n",
    "        all_vocab += vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now merge with existing vocab csv..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 1307 new words. Total vocab size: 1307.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def merge_vocab_list_to_csv(vocab_list, csv_file):\n",
    "    # Load existing vocab if the file exists\n",
    "    if os.path.exists(csv_file):\n",
    "        df = pd.read_csv(csv_file)\n",
    "    else:\n",
    "        df = pd.DataFrame(columns=['Entry', 'Notes', 'Mastery Score'])\n",
    "\n",
    "    # Normalize: remove empty strings and duplicates in incoming vocab list\n",
    "    vocab_set = set(filter(None, vocab_list))\n",
    "    existing_entries = set(df['Entry'].astype(str))\n",
    "\n",
    "    # Find new words to add\n",
    "    new_words = vocab_set - existing_entries\n",
    "    new_rows = pd.DataFrame({\n",
    "        'Entry': list(new_words),\n",
    "        'Notes': '',\n",
    "        'Mastery Score': 0\n",
    "    })\n",
    "\n",
    "    # Append and remove any accidental duplicates\n",
    "    combined_df = pd.concat([df, new_rows], ignore_index=True).drop_duplicates(subset=['Entry'])\n",
    "\n",
    "    # Save back to CSV\n",
    "    combined_df.to_csv(csv_file, index=False, encoding='utf-8')\n",
    "    print(f\"Added {len(new_words)} new words. Total vocab size: {len(combined_df)}.\")\n",
    "\n",
    "vocab_csv = 'vocab.csv'\n",
    "merge_vocab_list_to_csv(all_vocab, vocab_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now time for the chatbot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
